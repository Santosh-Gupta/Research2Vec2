{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PublicResearch2vec15MWSpeedTorchStart_9-15-19.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosh-Gupta/Research2Vec2/blob/master/PublicResearch2vec15MWSpeedTorchStart_9_15_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01LJQq_Tzo7b",
        "colab_type": "code",
        "outputId": "0d017f09-bfe4-47d5-e522-b4c97cc118ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Sep 15 04:29:08 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OzXb-qkzwjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy\n",
        "import random \n",
        "import hashlib\n",
        "\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "numpy.random.seed(12345)\n",
        "\n",
        "use_cuda = True\n",
        "# if use_cuda and torch.cuda.is_available():\n",
        "#     net.cuda()\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "import hashlib\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from tempfile import gettempdir\n",
        "import zipfile\n",
        "import h5py\n",
        "import requests\n",
        "import pickle\n",
        "\n",
        "# import loguniform\n",
        "# from loguniform import LogUniform\n",
        "\n",
        "data_index = 0\n",
        "vocabulary_size = 14886544+1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM86t7g7KPcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3-HsIkiagRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from StylesSantosh\n",
        "download_file_from_google_drive('1-1_aIpBG1kusXIO5w4Z81khFIoil69q7', 'SemanticScholar14MFinal.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_SgVxPjblWx",
        "colab_type": "code",
        "outputId": "f137de97-1ec4-4763-eed5-77616052c703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "#From StylesSantosh gdrive \n",
        "h5f2 = h5py.File('SemanticScholar14MFinal.h5','r')\n",
        "trainData = np.column_stack(( h5f2['labels'+str(0) ][:] , h5f2['batch'+str(0) ][:] ))\n",
        "for i in range(1, 235):\n",
        "    if i%10==0:\n",
        "        print(i)\n",
        "    toAppend2 = np.column_stack(( h5f2['labels'+str(i) ][:] , h5f2['batch'+str(i) ][:] ))\n",
        "    trainData = np.concatenate((trainData, toAppend2))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkzYlgFmJ6sN",
        "colab_type": "code",
        "outputId": "c4e1de87-266f-4065-c14d-45af20362775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "trainData"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0,\n",
              "        array([ 4928722,  3922609, 14413953, 10103423,  8948498], dtype=int32)],\n",
              "       [1,\n",
              "        array([12557217,  5572869, 13415223,  2532000, 14609022,  9830632,\n",
              "        9800679,  7504595, 10752682], dtype=int32)],\n",
              "       [2,\n",
              "        array([10458710,  7176517, 10268240,  4173086,  8617671,  4674075,\n",
              "       12580461,  2434641,  3694004,  9734870,  1314108,  8879955,\n",
              "        6468499, 12092464,  2962425, 13680848, 10590392, 10203584,\n",
              "       12816205,  7484678,  7985600, 12896218, 14882024,  6783345,\n",
              "         969850, 10709191,  4541728,  4312270,  6174902,   530425,\n",
              "        4843145,  4838613, 11404068,  9900162, 10578750, 12955180,\n",
              "        4602929,  4097386,  8870275,  7518195, 11849786,  2947773,\n",
              "       11653892,  7599644,  5895991,  1381764,  5853764, 11048535,\n",
              "       14128229, 11490202,   954680, 11998906,  9196156,  4506953,\n",
              "        6597761,  7034485,  3008940,  9816877,  1748801, 10159466,\n",
              "        2745090, 14842579,   788308,  5984365], dtype=int32)],\n",
              "       ...,\n",
              "       [14886540,\n",
              "        array([10834006, 14032200,  6336751,  6441125,  9893491, 14529509,\n",
              "       11166629, 12719271, 10359746, 11801213,  6736288,  8829703,\n",
              "       12003237, 12799587, 10167963,  3752873, 13215295,  5264102,\n",
              "       14711387,  8973945,  1047341,  6996480,  1347760, 10703628,\n",
              "        5757224,  1903484,  8548193,  9465902,   284331, 14489405,\n",
              "       14337590, 12270241, 12252782,  1628607, 10378253, 10221082,\n",
              "        3040746,  3137696,   161876], dtype=int32)],\n",
              "       [14886541,\n",
              "        array([11139528, 12138115, 13194943, 12722279,   946665,  9973659],\n",
              "      dtype=int32)],\n",
              "       [14886542,\n",
              "        array([11986890, 12069475,  3867780,  6851418,  7084076, 12512929,\n",
              "         480455,  6192598,  1564955,  7552789,  6753362, 10633540,\n",
              "        3717063, 10794203,  1120993,  4348485,  5720101,  4719797,\n",
              "        7858928, 10771131,  5418259,  3253039, 11141264,  5669541,\n",
              "       11097018,   758506,  9665957, 12531957,   799683,  3995019,\n",
              "        8932362, 11719919,  2685322, 10157005,  8693211,   985969,\n",
              "        7406973,  7307979,  1468328, 13542014, 13553332,  4521421,\n",
              "       13354693,  4979730,  7936696,  8731940,  2671683,  1154602,\n",
              "        5391006,  3287185, 13301776,  4831737,  4909437, 11220532,\n",
              "       13841020, 12582795,  1637191,  9483192], dtype=int32)]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiRJkUeRKm8B",
        "colab_type": "code",
        "outputId": "5e5177ac-eae5-4b0a-fcfa-a0aa605bfbf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(trainData)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14886543"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1egtNXMAC-B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del h5f2\n",
        "del toAppend2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWUfvGFFb-fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_index = 0\n",
        "epoch_index = 0\n",
        "recEpoch_indexA = 0 #Used to help keep store of the total number of epoches with the models\n",
        "\n",
        "def generate_batch(batch_size, inputCount, negRate): #batch size = number of labels\n",
        "  #inputCount = number of inputs per label\n",
        "    global data_index, epoch_index\n",
        "    \n",
        "    batch = np.ndarray(shape=(batch_size, inputCount), dtype=np.int32) \n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    negs = np.zeros(shape=(batch_size, negRate), dtype=np.int32)\n",
        "    \n",
        "    n=0\n",
        "    while n < batch_size:\n",
        "        if len( set(trainData[data_index, 1]) ) >= inputCount:\n",
        "            labels[n,0] = trainData[data_index, 0]\n",
        "            batch[n] = random.sample( set(trainData[data_index, 1]),  inputCount)\n",
        "            negs[n] = np.random.randint(vocabulary_size-1, size=(negRate))\n",
        "            n = n+1\n",
        "            data_index = (data_index + 1) % len(trainData) #may have to do something like len my_data[:]\n",
        "            if data_index == 0:\n",
        "                epoch_index = epoch_index + 1\n",
        "                print('Completed %d Epochs' % epoch_index)\n",
        "        else:\n",
        "            data_index = (data_index + 1) % len(trainData)\n",
        "            if data_index == 0:\n",
        "                epoch_index = epoch_index + 1\n",
        "                print('Completed %d Epochs' % epoch_index)\n",
        "    \n",
        "    return batch, labels, negs    \n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GABzscHCeZe8",
        "colab_type": "code",
        "outputId": "f4d0b2c6-3aef-4f89-e0df-a0653edeef60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        }
      },
      "source": [
        "here, goes, everything = generate_batch(20, 4, 64 ) # to do next, insert %len(headernumber)\n",
        "print('batch', here)\n",
        "print('labels', goes)\n",
        "print('negz', everything)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch [[10103423  3922609  8948498 14413953]\n",
            " [ 2532000 10752682  7504595 13415223]\n",
            " [12816205  1381764  7176517 12092464]\n",
            " [ 3074350  1128725  4827272  4921599]\n",
            " [ 2846719  7023031  3092258  2272346]\n",
            " [ 1939211 11884329  7809030 13839635]\n",
            " [ 2339991 13216270  5977433  2795956]\n",
            " [ 9404814  6280473  8676396 14538735]\n",
            " [12239357 14417780  9824874  8441841]\n",
            " [11304775  9926763   900066  8420575]\n",
            " [  542740 10539996  1306472 11471303]\n",
            " [ 4465189   446158     2631 13821902]\n",
            " [  518820  9523923  7587484  8259552]\n",
            " [ 4577472 10387099 13296465  4573826]\n",
            " [ 3574367 12851771   355002  6393533]\n",
            " [  219571 13879984 14728831  6123482]\n",
            " [ 8142630  5566911  2574866 12513727]\n",
            " [10857835 12669767   843401  8115839]\n",
            " [ 6651810 14323956  4378774  3517434]\n",
            " [11426641  8540632  8250326  9215494]]\n",
            "labels [[ 0]\n",
            " [ 1]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [ 4]\n",
            " [ 5]\n",
            " [ 6]\n",
            " [ 7]\n",
            " [ 9]\n",
            " [10]\n",
            " [11]\n",
            " [12]\n",
            " [13]\n",
            " [14]\n",
            " [15]\n",
            " [16]\n",
            " [17]\n",
            " [19]\n",
            " [20]\n",
            " [21]]\n",
            "negz [[14757349  7735425  1396132 ...  4416910 12101787 11235672]\n",
            " [ 9695200  5052139  2397770 ...  8874614  7314213 13860972]\n",
            " [ 9266301   232038 14142766 ... 12685273   865512   854553]\n",
            " ...\n",
            " [ 5583741  2476913 10424811 ...  2281668 10521569  2502362]\n",
            " [13232078  5108832  2516832 ...  8663723 14546559   681128]\n",
            " [ 2836374  4705807  2634356 ...  6042828  9011716 10163313]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pIU-wdPSvHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"Skip gram model of word2vec.\n",
        "    Attributes:\n",
        "        emb_size: Embedding size.\n",
        "        emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        u_embedding: Embedding for center word.\n",
        "        v_embedding: Embedding for neibor words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        \"\"\"Initialize model parameters.\n",
        "        Apply for two embedding layers.\n",
        "        Initialize layer weight\n",
        "        Args:\n",
        "            emb_size: Embedding size.\n",
        "            emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=False).cuda()\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=False).cuda()\n",
        "        # self.targets = torch.ones(34816).cud\n",
        "        self.init_emb()\n",
        "\n",
        "    def init_emb(self):\n",
        "        \"\"\"Initialize embedding weight like word2vec.\n",
        "        The u_embedding is a uniform distribution in [-0.5/em_size, 0.5/emb_size], and the elements of v_embedding are zeroes.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        initrange = (2.0 / (vocabulary_size + self.emb_dimension)) ** 0.5 # Xavier init\n",
        "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embeddings.weight.data.normal_(mean=0, std=math.sqrt(initrange))\n",
        "        self.lossFunction = nn.BCEWithLogitsLoss( reduction = 'none' )\n",
        "        \n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v, targets ):\n",
        "        \"\"\"Forward process.\n",
        "        As pytorch designed, all variables must be batch format, so all input of this method is a list of word id.\n",
        "        Args:\n",
        "            pos_u: list of center word ids for positive word pairs.\n",
        "            pos_v: list of neibor word ids for positive word pairs.\n",
        "            neg_u: list of center word ids for negative word pairs.\n",
        "            neg_v: list of neibor word ids for negative word pairs.\n",
        "        Returns:\n",
        "            Loss of this process, a pytorch variable.\n",
        "        \"\"\"\n",
        "        emb_u = self.u_embeddings(pos_u)\n",
        "        emb_v = self.v_embeddings(pos_v)\n",
        "        neg_emb_v = self.v_embeddings(neg_v)\n",
        "\n",
        "        scorePos = torch.bmm(emb_u, torch.transpose(emb_v, 1, 2)).squeeze()\n",
        "        scoreNeg = torch.bmm( emb_u , torch.transpose(neg_emb_v, 1, 2) ).squeeze()\n",
        "\n",
        "        #reduce to dot products for each set, and concatinate all the losses\n",
        "        #noticed that the sign change for the negative sample dot products\n",
        "        totalScores = torch.cat( ( scorePos.reshape(scorePos.shape[0], -1) , -scoreNeg.reshape(scoreNeg.shape[0], -1) ) , dim=1)\n",
        "\n",
        "        indLoss = self.lossFunction( totalScores, targets )\n",
        "        rowSum = torch.sum(indLoss, dim=1) #Sum all losses for each set\n",
        "        finalLoss = torch.mean(rowSum) #Average losses across batches \n",
        "\n",
        "        return finalLoss\n",
        "\n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"Save all embeddings to file.\n",
        "        As this class only record word id, so the map from id to word has to be transfered from outside.\n",
        "        Args:\n",
        "            id2word: map from word id to word.\n",
        "            file_name: file name.\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        if use_cuda:\n",
        "            embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embeddings.weight.data.numpy()\n",
        "        fout = open(file_name, 'w')\n",
        "        fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "        for wid, w in id2word.items():\n",
        "            e = embedding[wid]\n",
        "            e = ' '.join(map(lambda x: str(x), e))\n",
        "            fout.write('%s %s\\n' % (w, e))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXuNQUZrKV_A",
        "colab_type": "code",
        "outputId": "4ced5de3-143c-4014-f521-4b17754b0ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "2829853*496*2/14886544"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "188.57393468893787"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AD6fktamAlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_file_name = 'theOutpule.file'\n",
        "batch_size = 512\n",
        "window_size = 4\n",
        "negSamp = 132\n",
        "iterationsMax = 100000 #200001\n",
        "initial_lr =1.0\n",
        "\n",
        "num_classes = batch_size\n",
        "emb_dimension = 188\n",
        "\n",
        "skip_gram_model = SkipGramModel(num_classes, emb_dimension)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "if use_cuda:\n",
        "    skip_gram_model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    skip_gram_model.parameters(),  lr=0.01, momentum=0.9 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjz7wfhvQfP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(\n",
        "    skip_gram_model.parameters(),  lr=0.1 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZxPfq-KNZ9z",
        "colab_type": "code",
        "outputId": "3bad3f08-6646-462c-969c-31620ae15dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "!pip install SpeedTorch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting SpeedTorch\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/10/f077545535867f11da6dd007de18c6e07494684f2c15eb908661e1c7a306/SpeedTorch-0.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from SpeedTorch) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from SpeedTorch) (1.16.5)\n",
            "Installing collected packages: SpeedTorch\n",
            "Successfully installed SpeedTorch-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SOL8rvDNL1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import SpeedTorch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-wjhCzCNqVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The total u embeddings stay on the GPU, the total v embeddings stay on the CPU\n",
        "\n",
        "uEmbed_switcher = SpeedTorch.GPUPytorchModelFactory( skip_gram_model.u_embeddings, total_classes=vocabulary_size, embed_dimension=emb_dimension )\n",
        "vEmbed_switcher = SpeedTorch.ModelFactory( skip_gram_model.v_embeddings, total_classes=vocabulary_size, embed_dimension=emb_dimension, CPUPinn = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPdtzKI1NqSs",
        "colab_type": "code",
        "outputId": "1d0931e3-0caa-463e-be7d-452d896ec293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "initrange = (2.0 / (10000 + emb_dimension)) ** 0.5 # Xavier init\n",
        "uEmbed_switcher.uniformDistributionInit( -initrange, initrange )\n",
        "print('completed first')\n",
        "vEmbed_switcher.normalDistributionInit( 0, math.sqrt(initrange) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed first\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XZF097eNYAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 512\n",
        "labelsTranfored2 = uEmbed_switcher.variableTransformer( batchSize=batch_size, posPerBatch=1)\n",
        "batchTransformed2, negzTransformed2 = vEmbed_switcher.variableTransformer( batchSize=batch_size, posPerBatch=4, negPerBatch=132 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbo7Lm9geM4o",
        "colab_type": "code",
        "outputId": "f4d43019-e9e2-4f74-a229-58ed7209f446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "runningLoss = 0\n",
        "batch_size = 512\n",
        "negSamp = 132\n",
        "window_size = 4\n",
        "\n",
        "targets = torch.ones( batch_size, window_size + negSamp , dtype = torch.float32 ).cuda()\n",
        "\n",
        "batchTensor = torch.from_numpy(batchTransformed2)\n",
        "LabelTensor = torch.from_numpy(labelsTranfored2)\n",
        "negTensor = torch.from_numpy(negzTransformed2)\n",
        "\n",
        "pos_u = Variable(torch.LongTensor(LabelTensor.long()))\n",
        "pos_v = Variable(torch.LongTensor(batchTensor.long()))\n",
        "neg_v = Variable(torch.LongTensor(negTensor.long()))\n",
        "\n",
        "if use_cuda:\n",
        "    pos_u = pos_u.cuda()\n",
        "    pos_v = pos_v.cuda()\n",
        "    neg_v = neg_v.cuda()\n",
        "\n",
        "for i in range(100000):\n",
        "\n",
        "    batch, labels, negz = generate_batch(batch_size=batch_size, inputCount=window_size, negRate= negSamp)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        uEmbed_switcher.beforeForwardPass( labels )\n",
        "        vEmbed_switcher.beforeForwardPass( batch, negz )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = skip_gram_model.forward(pos_u, pos_v, neg_v, targets)\n",
        "    runningLoss = runningLoss + loss.data.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        uEmbed_switcher.afterOptimizerStep( labels )\n",
        "        vEmbed_switcher.afterOptimizerStep( batch, negz )\n",
        "\n",
        "    if i%1000 == 0:\n",
        "        print('i is ', i)\n",
        "        print('loss is ', runningLoss/1000)\n",
        "        runningLoss = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i is  0\n",
            "loss is  0.1885360412597656\n",
            "i is  500\n",
            "loss is  94.26802062988281\n",
            "i is  1000\n",
            "loss is  94.26802062988281\n",
            "i is  1500\n",
            "loss is  94.26802062988281\n",
            "i is  2000\n",
            "loss is  94.26802062988281\n",
            "i is  2500\n",
            "loss is  94.26802062988281\n",
            "i is  3000\n",
            "loss is  94.26802062988281\n",
            "i is  3500\n",
            "loss is  94.26802062988281\n",
            "i is  4000\n",
            "loss is  94.26802062988281\n",
            "i is  4500\n",
            "loss is  94.26802062988281\n",
            "i is  5000\n",
            "loss is  94.26802062988281\n",
            "i is  5500\n",
            "loss is  94.26802062988281\n",
            "i is  6000\n",
            "loss is  94.26802062988281\n",
            "i is  6500\n",
            "loss is  94.26802062988281\n",
            "i is  7000\n",
            "loss is  94.26802062988281\n",
            "i is  7500\n",
            "loss is  94.26802062988281\n",
            "Completed 1 Epochs\n",
            "i is  8000\n",
            "loss is  94.2680064239502\n",
            "i is  8500\n",
            "loss is  94.26799905395508\n",
            "i is  9000\n",
            "loss is  94.2680001373291\n",
            "i is  9500\n",
            "loss is  94.26799954223632\n",
            "i is  10000\n",
            "loss is  94.2679994354248\n",
            "i is  10500\n",
            "loss is  94.26800068664551\n",
            "i is  11000\n",
            "loss is  94.26799995422363\n",
            "i is  11500\n",
            "loss is  94.2680011138916\n",
            "i is  12000\n",
            "loss is  94.2679985961914\n",
            "i is  12500\n",
            "loss is  94.26800001525879\n",
            "i is  13000\n",
            "loss is  94.2679998626709\n",
            "i is  13500\n",
            "loss is  94.2679990234375\n",
            "i is  14000\n",
            "loss is  94.26800019836426\n",
            "i is  14500\n",
            "loss is  94.26799975585938\n",
            "i is  15000\n",
            "loss is  94.2679998626709\n",
            "i is  15500\n",
            "loss is  94.2679995880127\n",
            "i is  16000\n",
            "loss is  94.26800065612792\n",
            "i is  16500\n",
            "loss is  94.26800022888183\n",
            "i is  17000\n",
            "loss is  94.26800065612792\n",
            "i is  17500\n",
            "loss is  94.26800018310547\n",
            "i is  18000\n",
            "loss is  94.26799996948242\n",
            "i is  18500\n",
            "loss is  94.26800004577636\n",
            "i is  19000\n",
            "loss is  94.26799887084961\n",
            "i is  19500\n",
            "loss is  94.26799945068359\n",
            "i is  20000\n",
            "loss is  94.26800015258789\n",
            "i is  20500\n",
            "loss is  94.26799975585938\n",
            "i is  21000\n",
            "loss is  94.26799919128418\n",
            "i is  21500\n",
            "loss is  94.26799891662597\n",
            "i is  22000\n",
            "loss is  94.26799920654297\n",
            "i is  22500\n",
            "loss is  94.26800035095215\n",
            "i is  23000\n",
            "loss is  94.26800010681153\n",
            "i is  23500\n",
            "loss is  94.26800045776368\n",
            "i is  24000\n",
            "loss is  94.26799935913085\n",
            "i is  24500\n",
            "loss is  94.26800006103515\n",
            "i is  25000\n",
            "loss is  94.26794161987304\n",
            "i is  25500\n",
            "loss is  94.26785708618164\n",
            "i is  26000\n",
            "loss is  94.26785057067872\n",
            "i is  26500\n",
            "loss is  94.26784973144531\n",
            "i is  27000\n",
            "loss is  94.26784959411621\n",
            "i is  27500\n",
            "loss is  94.26785020446778\n",
            "i is  28000\n",
            "loss is  94.26785130310058\n",
            "i is  28500\n",
            "loss is  94.26785174560547\n",
            "i is  29000\n",
            "loss is  94.26785137939453\n",
            "i is  29500\n",
            "loss is  94.26785316467286\n",
            "i is  30000\n",
            "loss is  94.26784965515137\n",
            "i is  30500\n",
            "loss is  94.2678521270752\n",
            "i is  31000\n",
            "loss is  94.26784844970703\n",
            "i is  31500\n",
            "loss is  94.2678526763916\n",
            "i is  32000\n",
            "loss is  94.26785317993163\n",
            "Completed 2 Epochs\n",
            "i is  32500\n",
            "loss is  94.26784951782227\n",
            "i is  33000\n",
            "loss is  94.26783758544921\n",
            "i is  33500\n",
            "loss is  94.2678367767334\n",
            "i is  34000\n",
            "loss is  94.26783792114257\n",
            "i is  34500\n",
            "loss is  94.26783236694335\n",
            "i is  35000\n",
            "loss is  94.26783636474609\n",
            "i is  35500\n",
            "loss is  94.26783532714843\n",
            "i is  36000\n",
            "loss is  94.26783598327637\n",
            "i is  36500\n",
            "loss is  94.2678348083496\n",
            "i is  37000\n",
            "loss is  94.26783682250976\n",
            "i is  37500\n",
            "loss is  94.26783125305175\n",
            "i is  38000\n",
            "loss is  94.26783894348145\n",
            "i is  38500\n",
            "loss is  94.26783348083497\n",
            "i is  39000\n",
            "loss is  94.26783384704589\n",
            "i is  39500\n",
            "loss is  94.26783712768555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g-hdHYMCwZA",
        "colab_type": "text"
      },
      "source": [
        "# After training recommended lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cydih-XIOvkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete data to free up RAM \n",
        "\n",
        "del trainData\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzVizvxXD7RV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save embeddings directly. Warning, the cupy version will take a while, and needs additional ram\n",
        "\n",
        "vEmbed_switcher.saveCupy('filenameU.cpy')\n",
        "\n",
        "uEmbed_switcher.saveTorch('filenameV.torch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRzWIGx9DhaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get numpy version of u embeddings\n",
        "\n",
        "numpyU = uEmbed_switcher.getNumpyVersion()\n",
        "\n",
        "numpyV = uEmbed_switcher.getNumpyVersion()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFYGv717ZH2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize embedings, set up a few random papers, and look for similiar papers\n",
        "\n",
        "normalizedU = np.divide( numpyU, np.sqrt( np.sum(np.square(numpyU), axis=1) )[:,None] )\n",
        "\n",
        "testIndexes = np.array(range(20, 100)) \n",
        "TestEmbeds = normalizedSum[testIndexes]\n",
        "simNumpy = np.matmul(TestEmbeds, normalizedSum.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rkWnvtVgYM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_VKwqNQbBDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get dictionaries\n",
        "\n",
        "download_file_from_google_drive('1-BVhAO252myZW2U6-iJ-v3vxr6zaASv3', 'Index2Title.pickle')\n",
        "download_file_from_google_drive('1-BAaws04G0LH_o-L23LVtj0QSpn4yEY5', 'Index2ID.pickle')\n",
        "\n",
        "with open('Index2Title.pickle', 'rb') as handle:\n",
        "    Index2Title = pickle.load(handle)\n",
        "\n",
        "with open('Index2ID.pickle', 'rb') as handle:\n",
        "    Index2ID = pickle.load(handle)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwS0tdfRguxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c92cf01e-20c2-44e2-f878-055bd2829a07"
      },
      "source": [
        "Index2Title[333]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Extent and modulation of junctional communication between pancreatic acinar cells in vivo.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azSNYb2pg4tl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "273c6aa4-4a6c-4298-90b0-ce54999f5cec"
      },
      "source": [
        "Index2ID[333]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'6ce85560385a968959f3ac28af40a27abc03137c'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_86Mcd5KhEPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7f7bdd6b-e1c8-49cd-81ee-71afe80b9e9a"
      },
      "source": [
        "print( 'Link is https://www.semanticscholar.org/paper/%s' % Index2ID[333])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Link is https://www.semanticscholar.org/paper/6ce85560385a968959f3ac28af40a27abc03137c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjnVP9LHeiK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at similiar papers  \n",
        "\n",
        "for i in range(len(testIndexes)):\n",
        "    paper = Index2Title[i]\n",
        "    top_k = 80  # number of nearest neighbors\n",
        "    nearest = (-simNumpy[i, :]).argsort()[1:top_k + 1]\n",
        "    log_str = 'Nearest to %s:' % paper\n",
        "    for k in xrange(top_k):\n",
        "        close_paper = Index2Title[nearest[k]]\n",
        "        linkID = Index2ID[nearest[k]]\n",
        "        log_str = ' %s %s,' % (log_str, close_paper)\n",
        "        log_str = '%s Link https://www.semanticscholar.org/paper/%s  ' % (log_str, linkID)\n",
        "    print(log_str)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}