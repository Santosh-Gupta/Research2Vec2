{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PublicResearch2vec15MWRegStart_9-15-19.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosh-Gupta/Research2Vec2/blob/master/PublicResearch2vec15MWRegStart_9_15_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01LJQq_Tzo7b",
        "colab_type": "code",
        "outputId": "996e2ef0-366e-446f-c221-4bf70a664e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Sep 15 20:57:23 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OzXb-qkzwjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy\n",
        "import random \n",
        "import hashlib\n",
        "\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "numpy.random.seed(12345)\n",
        "\n",
        "use_cuda = True\n",
        "# if use_cuda and torch.cuda.is_available():\n",
        "#     net.cuda()\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "import hashlib\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from tempfile import gettempdir\n",
        "import zipfile\n",
        "import h5py\n",
        "import requests\n",
        "import pickle\n",
        "\n",
        "# import loguniform\n",
        "# from loguniform import LogUniform\n",
        "\n",
        "data_index = 0\n",
        "vocabulary_size = 14886544+1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM86t7g7KPcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3-HsIkiagRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from StylesSantosh\n",
        "download_file_from_google_drive('1-1_aIpBG1kusXIO5w4Z81khFIoil69q7', 'SemanticScholar14MFinal.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_SgVxPjblWx",
        "colab_type": "code",
        "outputId": "e715279f-e439-48ff-fdf4-8d20789b7119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "#From StylesSantosh gdrive \n",
        "h5f2 = h5py.File('SemanticScholar14MFinal.h5','r')\n",
        "trainData = np.column_stack(( h5f2['labels'+str(0) ][:] , h5f2['batch'+str(0) ][:] ))\n",
        "for i in range(1, 235):\n",
        "    if i%10==0:\n",
        "        print(i)\n",
        "    toAppend2 = np.column_stack(( h5f2['labels'+str(i) ][:] , h5f2['batch'+str(i) ][:] ))\n",
        "    trainData = np.concatenate((trainData, toAppend2))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkzYlgFmJ6sN",
        "colab_type": "code",
        "outputId": "692aef1f-18da-4ebe-8dd0-30cdb833ead5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "trainData"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0,\n",
              "        array([ 4928722,  3922609, 14413953, 10103423,  8948498], dtype=int32)],\n",
              "       [1,\n",
              "        array([12557217,  5572869, 13415223,  2532000, 14609022,  9830632,\n",
              "        9800679,  7504595, 10752682], dtype=int32)],\n",
              "       [2,\n",
              "        array([10458710,  7176517, 10268240,  4173086,  8617671,  4674075,\n",
              "       12580461,  2434641,  3694004,  9734870,  1314108,  8879955,\n",
              "        6468499, 12092464,  2962425, 13680848, 10590392, 10203584,\n",
              "       12816205,  7484678,  7985600, 12896218, 14882024,  6783345,\n",
              "         969850, 10709191,  4541728,  4312270,  6174902,   530425,\n",
              "        4843145,  4838613, 11404068,  9900162, 10578750, 12955180,\n",
              "        4602929,  4097386,  8870275,  7518195, 11849786,  2947773,\n",
              "       11653892,  7599644,  5895991,  1381764,  5853764, 11048535,\n",
              "       14128229, 11490202,   954680, 11998906,  9196156,  4506953,\n",
              "        6597761,  7034485,  3008940,  9816877,  1748801, 10159466,\n",
              "        2745090, 14842579,   788308,  5984365], dtype=int32)],\n",
              "       ...,\n",
              "       [14886540,\n",
              "        array([10834006, 14032200,  6336751,  6441125,  9893491, 14529509,\n",
              "       11166629, 12719271, 10359746, 11801213,  6736288,  8829703,\n",
              "       12003237, 12799587, 10167963,  3752873, 13215295,  5264102,\n",
              "       14711387,  8973945,  1047341,  6996480,  1347760, 10703628,\n",
              "        5757224,  1903484,  8548193,  9465902,   284331, 14489405,\n",
              "       14337590, 12270241, 12252782,  1628607, 10378253, 10221082,\n",
              "        3040746,  3137696,   161876], dtype=int32)],\n",
              "       [14886541,\n",
              "        array([11139528, 12138115, 13194943, 12722279,   946665,  9973659],\n",
              "      dtype=int32)],\n",
              "       [14886542,\n",
              "        array([11986890, 12069475,  3867780,  6851418,  7084076, 12512929,\n",
              "         480455,  6192598,  1564955,  7552789,  6753362, 10633540,\n",
              "        3717063, 10794203,  1120993,  4348485,  5720101,  4719797,\n",
              "        7858928, 10771131,  5418259,  3253039, 11141264,  5669541,\n",
              "       11097018,   758506,  9665957, 12531957,   799683,  3995019,\n",
              "        8932362, 11719919,  2685322, 10157005,  8693211,   985969,\n",
              "        7406973,  7307979,  1468328, 13542014, 13553332,  4521421,\n",
              "       13354693,  4979730,  7936696,  8731940,  2671683,  1154602,\n",
              "        5391006,  3287185, 13301776,  4831737,  4909437, 11220532,\n",
              "       13841020, 12582795,  1637191,  9483192], dtype=int32)]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiRJkUeRKm8B",
        "colab_type": "code",
        "outputId": "c403f2e7-7ef4-4333-c999-fb47f869b498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(trainData)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14886543"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1egtNXMAC-B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del h5f2\n",
        "del toAppend2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWUfvGFFb-fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_index = 0\n",
        "epoch_index = 0\n",
        "recEpoch_indexA = 0 #Used to help keep store of the total number of epoches with the models\n",
        "\n",
        "def generate_batch(batch_size, inputCount, negRate): #batch size = number of labels\n",
        "  #inputCount = number of inputs per label\n",
        "    global data_index, epoch_index\n",
        "    \n",
        "    batch = np.ndarray(shape=(batch_size, inputCount), dtype=np.int32) \n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    negs = np.zeros(shape=(batch_size, negRate), dtype=np.int32)\n",
        "    \n",
        "    n=0\n",
        "    while n < batch_size:\n",
        "        if len( set(trainData[data_index, 1]) ) >= inputCount:\n",
        "            labels[n,0] = trainData[data_index, 0]\n",
        "            batch[n] = random.sample( set(trainData[data_index, 1]),  inputCount)\n",
        "            negs[n] = np.random.randint(vocabulary_size-1, size=(negRate))\n",
        "            n = n+1\n",
        "            data_index = (data_index + 1) % len(trainData) #may have to do something like len my_data[:]\n",
        "            if data_index == 0:\n",
        "                epoch_index = epoch_index + 1\n",
        "                print('Completed %d Epochs' % epoch_index)\n",
        "        else:\n",
        "            data_index = (data_index + 1) % len(trainData)\n",
        "            if data_index == 0:\n",
        "                epoch_index = epoch_index + 1\n",
        "                print('Completed %d Epochs' % epoch_index)\n",
        "    \n",
        "    return batch, labels, negs    \n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GABzscHCeZe8",
        "colab_type": "code",
        "outputId": "cd97e6f5-f44e-4362-e53c-8a7072964f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        }
      },
      "source": [
        "here, goes, everything = generate_batch(20, 4, 64 ) # to do next, insert %len(headernumber)\n",
        "print('batch', here)\n",
        "print('labels', goes)\n",
        "print('negz', everything)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch [[ 8948498  4928722 14413953 10103423]\n",
            " [ 2532000  7504595  5572869 13415223]\n",
            " [ 6597761  6783345  4541728  9816877]\n",
            " [ 4827272  4921599  7784960 11880992]\n",
            " [ 7023031  2846719  3092258 11002632]\n",
            " [13839635  2150715  3603537  7809030]\n",
            " [ 5977433 14788864  1919631  8601049]\n",
            " [ 9892408  8394768  7632793  8676396]\n",
            " [ 2698343  7744032 11733103  8441841]\n",
            " [12893030   900066  8420575 13210535]\n",
            " [ 1339707  1306472   678845  7569515]\n",
            " [ 7220937 13548319  3747359  9803338]\n",
            " [ 9523923 12100596 10697939 11615708]\n",
            " [11737196 10995808  6768877  7063482]\n",
            " [14623117 10136028  9097163  8335481]\n",
            " [14044866  7718879 10322151   219571]\n",
            " [ 5566911  6068411 10124958  8706402]\n",
            " [ 3377369 12811871  1869876   272482]\n",
            " [ 7167337  7740683 14323956  7163772]\n",
            " [11426641  9215494  4963905   763113]]\n",
            "labels [[ 0]\n",
            " [ 1]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [ 4]\n",
            " [ 5]\n",
            " [ 6]\n",
            " [ 7]\n",
            " [ 9]\n",
            " [10]\n",
            " [11]\n",
            " [12]\n",
            " [13]\n",
            " [14]\n",
            " [15]\n",
            " [16]\n",
            " [17]\n",
            " [19]\n",
            " [20]\n",
            " [21]]\n",
            "negz [[14757349  7735425  1396132 ...  4416910 12101787 11235672]\n",
            " [ 9695200  5052139  2397770 ...  8874614  7314213 13860972]\n",
            " [ 9266301   232038 14142766 ... 12685273   865512   854553]\n",
            " ...\n",
            " [ 5583741  2476913 10424811 ...  2281668 10521569  2502362]\n",
            " [13232078  5108832  2516832 ...  8663723 14546559   681128]\n",
            " [ 2836374  4705807  2634356 ...  6042828  9011716 10163313]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pIU-wdPSvHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"Skip gram model of word2vec.\n",
        "    Attributes:\n",
        "        emb_size: Embedding size.\n",
        "        emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        u_embedding: Embedding for center word.\n",
        "        v_embedding: Embedding for neibor words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        \"\"\"Initialize model parameters.\n",
        "        Apply for two embedding layers.\n",
        "        Initialize layer weight\n",
        "        Args:\n",
        "            emb_size: Embedding size.\n",
        "            emb_dimention: Embedding dimention, typically from 50 to 500.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=False).cuda()\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=False).cuda()\n",
        "        # self.targets = torch.ones(34816).cud\n",
        "        self.init_emb()\n",
        "\n",
        "    def init_emb(self):\n",
        "        \"\"\"Initialize embedding weight like word2vec.\n",
        "        The u_embedding is a uniform distribution in [-0.5/em_size, 0.5/emb_size], and the elements of v_embedding are zeroes.\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        initrange = (2.0 / (vocabulary_size + self.emb_dimension)) ** 0.5 # Xavier init\n",
        "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embeddings.weight.data.normal_(mean=0, std=math.sqrt(initrange))\n",
        "        self.lossFunction = nn.BCEWithLogitsLoss( reduction = 'none' )\n",
        "        \n",
        "\n",
        "    def forward(self, pos_u, pos_v, neg_v, targets ):\n",
        "        \"\"\"Forward process.\n",
        "        As pytorch designed, all variables must be batch format, so all input of this method is a list of word id.\n",
        "        Args:\n",
        "            pos_u: list of center word ids for positive word pairs.\n",
        "            pos_v: list of neibor word ids for positive word pairs.\n",
        "            neg_u: list of center word ids for negative word pairs.\n",
        "            neg_v: list of neibor word ids for negative word pairs.\n",
        "        Returns:\n",
        "            Loss of this process, a pytorch variable.\n",
        "        \"\"\"\n",
        "        emb_u = self.u_embeddings(pos_u)\n",
        "        emb_v = self.v_embeddings(pos_v)\n",
        "        neg_emb_v = self.v_embeddings(neg_v)\n",
        "\n",
        "        scorePos = torch.bmm(emb_u, torch.transpose(emb_v, 1, 2)).squeeze()\n",
        "        scoreNeg = torch.bmm( emb_u , torch.transpose(neg_emb_v, 1, 2) ).squeeze()\n",
        "\n",
        "        #reduce to dot products for each set, and concatinate all the losses\n",
        "        #noticed that the sign change for the negative sample dot products\n",
        "        totalScores = torch.cat( ( scorePos.reshape(scorePos.shape[0], -1) , -scoreNeg.reshape(scoreNeg.shape[0], -1) ) , dim=1)\n",
        "\n",
        "        indLoss = self.lossFunction( totalScores, targets )\n",
        "        rowSum = torch.sum(indLoss, dim=1) #Sum all losses for each set\n",
        "        finalLoss = torch.mean(rowSum) #Average losses across batches \n",
        "\n",
        "        return finalLoss\n",
        "\n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"Save all embeddings to file.\n",
        "        As this class only record word id, so the map from id to word has to be transfered from outside.\n",
        "        Args:\n",
        "            id2word: map from word id to word.\n",
        "            file_name: file name.\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        if use_cuda:\n",
        "            embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embeddings.weight.data.numpy()\n",
        "        fout = open(file_name, 'w')\n",
        "        fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "        for wid, w in id2word.items():\n",
        "            e = embedding[wid]\n",
        "            e = ' '.join(map(lambda x: str(x), e))\n",
        "            fout.write('%s %s\\n' % (w, e))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AD6fktamAlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_file_name = 'theOutpule.file'\n",
        "batch_size = 512\n",
        "window_size = 4\n",
        "negSamp = 132\n",
        "iterationsMax = 100000 #200001\n",
        "initial_lr =1.0\n",
        "\n",
        "num_classes = vocabulary_size\n",
        "emb_dimension = 96\n",
        "\n",
        "skip_gram_model = SkipGramModel(vocabulary_size, emb_dimension)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "if use_cuda:\n",
        "    skip_gram_model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    skip_gram_model.parameters(),  lr=0.01, momentum=0.9 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbo7Lm9geM4o",
        "colab_type": "code",
        "outputId": "8e640d39-0061-4758-df31-7f28ec08de5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "runningLoss = 0\n",
        "batch_size = 512\n",
        "negSamp = 132\n",
        "window_size = 4\n",
        "\n",
        "targets = torch.ones( batch_size, window_size + negSamp , dtype = torch.float32 ).cuda()\n",
        "\n",
        "for i in range(100000):\n",
        "\n",
        "    batch, labels, negz = generate_batch(batch_size=batch_size, inputCount=window_size, negRate= negSamp)\n",
        "\n",
        "    batchTensor = torch.from_numpy(batch)\n",
        "    LabelTensor = torch.from_numpy(labels)\n",
        "    negTensor = torch.from_numpy(negz)\n",
        "\n",
        "    pos_u = Variable(torch.LongTensor(LabelTensor.long()))\n",
        "    pos_v = Variable(torch.LongTensor(batchTensor.long()))\n",
        "    neg_v = Variable(torch.LongTensor(negTensor.long()))\n",
        "\n",
        "    if use_cuda:\n",
        "        pos_u = pos_u.cuda()\n",
        "        pos_v = pos_v.cuda()\n",
        "        neg_v = neg_v.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = skip_gram_model.forward(pos_u, pos_v, neg_v, targets)\n",
        "    runningLoss = runningLoss + loss.data.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i%1000 == 0:\n",
        "        print('i is ', i)\n",
        "        print('loss is ', runningLoss/1000)\n",
        "        runningLoss = 0"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2857aa630766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mrunningLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunningLoss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.32 GiB (GPU 0; 11.17 GiB total capacity; 10.70 GiB already allocated; 142.81 MiB free; 3.94 MiB cached)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g-hdHYMCwZA",
        "colab_type": "text"
      },
      "source": [
        "# After training recommended lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cydih-XIOvkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete data to free up RAM \n",
        "\n",
        "del trainData\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFYGv717ZH2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize embedings, set up a few random papers, and look for similiar papers\n",
        "\n",
        "normalizedU = np.divide( numpyU, np.sqrt( np.sum(np.square(numpyU), axis=1) )[:,None] )\n",
        "\n",
        "testIndexes = np.array(range(20, 100)) \n",
        "TestEmbeds = normalizedSum[testIndexes]\n",
        "simNumpy = np.matmul(TestEmbeds, normalizedSum.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_VKwqNQbBDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get dictionaries\n",
        "\n",
        "download_file_from_google_drive('1-BVhAO252myZW2U6-iJ-v3vxr6zaASv3', 'Index2Title.pickle')\n",
        "download_file_from_google_drive('1-BAaws04G0LH_o-L23LVtj0QSpn4yEY5', 'Index2ID.pickle')\n",
        "\n",
        "with open('Index2Title.pickle', 'rb') as handle:\n",
        "    Index2Title = pickle.load(handle)\n",
        "\n",
        "with open('Index2ID.pickle', 'rb') as handle:\n",
        "    Index2ID = pickle.load(handle)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwS0tdfRguxz",
        "colab_type": "code",
        "outputId": "c92cf01e-20c2-44e2-f878-055bd2829a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "Index2Title[333]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Extent and modulation of junctional communication between pancreatic acinar cells in vivo.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azSNYb2pg4tl",
        "colab_type": "code",
        "outputId": "273c6aa4-4a6c-4298-90b0-ce54999f5cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "Index2ID[333]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'6ce85560385a968959f3ac28af40a27abc03137c'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_86Mcd5KhEPW",
        "colab_type": "code",
        "outputId": "7f7bdd6b-e1c8-49cd-81ee-71afe80b9e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print( 'Link is https://www.semanticscholar.org/paper/%s' % Index2ID[333])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Link is https://www.semanticscholar.org/paper/6ce85560385a968959f3ac28af40a27abc03137c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjnVP9LHeiK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at similiar papers  \n",
        "\n",
        "for i in range(len(testIndexes)):\n",
        "    paper = Index2Title[i]\n",
        "    top_k = 80  # number of nearest neighbors\n",
        "    nearest = (-simNumpy[i, :]).argsort()[1:top_k + 1]\n",
        "    log_str = 'Nearest to %s:' % paper\n",
        "    for k in xrange(top_k):\n",
        "        close_paper = Index2Title[nearest[k]]\n",
        "        linkID = Index2ID[nearest[k]]\n",
        "        log_str = ' %s %s,' % (log_str, close_paper)\n",
        "        log_str = '%s Link https://www.semanticscholar.org/paper/%s  ' % (log_str, linkID)\n",
        "    print(log_str)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}